All right. Hey guys, so I'm going to do a quick intro and then Drew is going to talk a little bit. So I wanted to just introduce Druba. Druba has done a ton of stuff in his career. about today is his work that he did at Facebook, which many of you guys know and used. Obviously, he helped build RocksDB. And since then, he has started a little stealth mode startup called Rockset, where he is the CTO. and co-founder. And now I'm going to hand it off to Juba. We'll have a little bit of time at the end, by the way, for questions. So think during the presentation about what you want to ask. Thank you, Brian. So thanks a lot for your time. folks for inviting me to talk to you today. Really appreciate it. It's always great to talk to users of software that we have built, because I always get new insights and new feedback into how to make this system better. So, today's In this talk, I have some slideware which I can cover, I'd rather take more questions from you. So if you have a question, please feel free to raise your hand Just ask me questions. Even if I don't cover all my slides, that's fine because I can share the slides with you later. and you can still read it. I'd rather like this talk to be more like a a discussion format rather than a one-sided thing. Sounds good? Just a little bit introduction about myself. I've been in the Valley for a long time. maybe like 25 years or so, writing different kinds of software. Started time when there was no and mostly writing product software. But the last two projects that I have done are on open source. So I have built a lot of open source software in the last 12 years, starting from a HDFS, the Hadoop file system, which I helped build when I was at Yahoo. And then RocksDB, which was when I was at Facebook. Right now, I am in a stealth mode startup, so I left Facebook around a year and a half back. and just trying to do something more on the cloud platform. and something that I can't talk about much today. Sorry. So a little bit background about Rocksteam. Some people sometimes ask me, like, why did you decide to do RocksDB, and, like, what was the motivation factor? So if you look at 2010 or some time around that, We used to have disk subsystems where we used to store all our data. When I'm talking we, that means at Facebook, because that's where I used to work there. So I used to store a lot of data on disks. And if you see, like the approximate times of network round trip and disk latencies. like 10 milliseconds versus 50 microseconds. Again, these numbers could be different from your network, I wanted to give you a little perspective of how different they are. So the database servers never really cared about the network round trip because most of the time it's taken accessing your data from the disks. So if your application is sitting here, it doesn't really care whether the database is very close to it or very far away from it because most of the time is in the 10 millisecond latencies of fetching data the disk. This is when disks were attached to the database servers, right? In In 2010, Facebook moved a lot of their database servers to SSDs, and so now you can see the the difference in latencies where SSD accesses are like, say, hundreds of microseconds. And so now the network is work latencies become maybe 50% of your total work of your total latency because you spend 50% microseconds here and 100 microseconds getting data from SSD. So the difference in my mind at that time was kind of getting clear saying that, hey, maybe we should build an embedded database which runs close to where the application is. If you remove the disks and the database server from there and then move it here directly to the application server. So this is my thesis at the time. There are disadvantages to this because you might not be able to ship database code and application code differently, right? So there are disadvantages to this, but if you're looking This is kind of really good because the application server Can directly access the server. database. So this is why I thought I should build embedded database. It was a... It was a project that I started just because my manager gave me like, leeway saying that, hey, go see if there's works. Somebody asked me today, like, how did you make this work, or how did you get started? There wasn't a grand plan our scheme of things, saying that, oh, this is going to be the next biggest thing on earth. Same thing I remember when I joined Yahoo and when I was doing the Hadoop file system. There was one intern who was working in the Yahoo indexing team, And he was the only guy I was working with. And nobody actually bothered anything about us. They just left us alone. guys, we don't know what they're doing, just let them be. So there were no grand plan of things in many of these open open source system development. Because these infrastructure development sometimes takes like five years before it becomes a really full-fledged product. Because of these embedded databases, this is how RocksDB was born in my mind. So what we needed was that a key value persistence store, which is what RocksDB is, and then it's embedded, and we want to optimize it for fast storage, which means that we want a system or a key value store which can work well when data is stored on SSDs or RAM. So that was the focus of the project. And then obviously server workloads, which means that we run it on machines which have probably eight or six Yeah, make sense? Any questions so far? So, RocksDB, it's definitely not It is not a distributed system, so it's a key values C++ library. Any of you use RocksDB? I think I know some people who use RocksDB. Yeah, so RocksDB is a C++ library. So it's not really like a database in a sense, a database management system. So there's no distributor, no failover, and it's not highly available. So if your data or your SSD catches fire, lose all your data unless you do your application yourself. So in your use cases that you use Rocksteam, Maybe, probably you already have some mechanism to replicate data from one node to another. Okay. And the focus, again, was a little bit more on the single node performance. So just to bring everybody up to speed, So the basic abstract is that keys and values. And keys and values are byte strings. There's no schema, or it's not like an SQL schema database where you have types of... of records and data is stored by key. So all your data, your entire database is basically a source sorted list of keys. And the operations are put delete and merge. So puts are when you add new data to the system. Deletes are when you delete a keyframe. And merge is an operation that I can explain more, which essentially lets you avoid read-modify-write. and implement kind of Redis lists and those kinds of advanced data structures. So There are three types of records that the system knows about internally, and the queries It's get, which means that given a key, fetch the value, and iterate, or scan. key, give me the values and go to the next key. So those are the only two operations for reading. There are many other operations that we added over time, but that's the basic. So the log structured margin engine So the so the rocks DB is a log structured margin engine, which is different from a traditional B-tree database. The log structure. merge architecture works this way so a write request comes from the application uh the the database has an in-memory copy of this data in RAM. It's called the memtable. or the memcache. So basically write this record into in-memory piece of data and also write it to a transaction log. So that's what happens when you have a write. rights go to memory buffer and to the transaction log. And then, now obviously So basically this memory buffer will fill up after some time, right? So you can't just keep writing everything to memory. At that time, you sort those memory buffers and write it to files in SSD or disk. essentially these get copied over into storage systems. And then you periodically compile pack this file so that you can keep this under reasonable space amplification, which means that you don't just run out of disk space, because that's the reason why you need to compact. Any questions so far? I can tell you how it kind of works for rocks DB in particular so that's the general-purpose LSM engine so write request comes in it writes writes to an active memtable, which is on top. And then, like I said, the write goes to a transaction log. So once this active memtable is full, this goes to become a read-only memtable, new rights go to new active memtables. So I think you were asking me the differences from what we did in the very early days from level DB, right? So this was like pipeline rights because there's disk subsystems where we wanted to have a pipeline of rights to these disks because there are disks that are taking time. Whereas, LevelDB had only like one active memtable. So these memtables get flushed out to new files on the storage, which are called SSD files. SSD stands for, like, sorted, what is it called? Sorted trees? Sorted string table, yeah. SST files, they're the data files, essentially, and then the logs that I mentioned here are essentially the transaction log. So it's append only, rights to the transaction law. And for the reason, Now if you look at the architecture, the reads are problematic because when you do a read, you might have to look at many SSD files to find where your key is, right? Which means that you have to have an efficient way of finding where your key is instead of looking at each and every SSD file. And so we use bloom filters to reduce the amount of IO. So the bloom filters, in my mind is a great invention just because it enables so many different types of technologies that use bloom filters now. So using bloom filters, you don't have to look at all the files to find your key. You can actually figure out which are the possible candidates, which are the possible candidate SSD files that has your key, and you look only at those. So the read- The code path is like this. So this is the write code path. Writes coming in and getting stored in SSD files, and they get compacted. So when the read comes in, like a get for a key, it first looks at the active memtable. If the key is there, it gets it from there, because that's the most recent version of the key. If it doesn't find, then it looks at all the only memtables and sees if the keys are there. The key that we are looking for. And if it is not there, then we need to go look at all the SSD files and find out if the key is there. The SSD files are kind of partitioned by key ranges, so it's easy to find. But still, you might have to look at more than one SSD file. And this is what I mean by read amplification, in the sense that when you're doing a read, you might have to do multiple reads under storage to find where the key is. But that overhead is reduced because we have this implementation of blooms. So using the bloom filters, we can actually narrow down the set of where the key is. Yeah, make sense? Any questions so far? yeah yeah so the bloom filters so every sst file has a bloom block here which is basically the bloom filters for that SST. So when you are doing a GET, most databases we configure such that the bloom filter block is in memory, but you can configure it to not be in memory and then it'll be paged into memory when it is needed And then these things that we read, for the read request, they actually get to something called the read-only block cache, so that in case you need to read the same key again, it fetches it from the block cache instead of going to find, instead of refinding it in the SSD file. So this is mostly an optimization. It's again in a piece of memory that we have. that basically caches for reads all the keys or some of the keys from your ssd5 Any questions so far? Oh yeah, good question. So the interesting thing in my mind in this LSM engine all these SSD files are read-only. Nobody updates SSD files. They just create new SSD files. So the entire database is actually all read-only, except this part which is the read-write part. That part. So this is a few megabytes, whereas your entire database could be, say, a terabyte of space, right? So now your question about invalidating it. Invalidating is mostly a question when you get a read request. You don't really need to invalidate everything you need to find the latest version of the key so there is a way so when when a request there is a way for us to figure out where, which files the latest version of the keys are. So we don't really delete them. The only time files get deleted is by the server. compaction process. So that's another advantage of an LSM engine where you can kind of scale out just because most of our database is read-only. Yes, question? Oh, great question. So every database has many different SST files, right? And all SST files can share this block cache. And you can also make this blocker share assistive files from another database. So you can have, let's say in a machine, you have four database instances, right? You can have one block cache, combined block cache for all the four instances, right? Just because again you don't want to fragment, this is kind of the biggest piece of memory in your system. machine has 64 gig, you might want to give 60 gig to this guy for mostly read-intensive see queries. So it is not fragmented, and you can share it among multiple database instances on the same same machine. set on per level basis. For example, I can set up . Yeah, so what happens is there's something called column families. Please hold on to this question for a second. When I talk about column families, then I'll explain how we can set different settings for different column families levels in the database. For example, like Z standard compression for the Yeah, there are many other tunables based on levels. For example, the ratio between levels. You can set it for every level. By default, it's 10, which means that the lowest level has, let's say the topmost level is one byte, the next level will have ten bytes. but you could change it for every level that you have. Block sizes and stuff like that, that it's not really per level. It's mostly for the database, but you can change the block sizes. and it is backward compatible. It's not going to recreate all the files. Again, the format for RocksDB, I think one of the big use cases or why rocks to be became very popular is because a lot of its parts are pluggable and adaptable and changeable, and they're well-defined APIs. So for example, the The way we write the transaction log, it's very customizable. So you can actually write it to your own transaction log. example suppose you want to write a transaction log to is a local disk you want to write it to Kafka or something else, some other systems that you have, you can very easily plug it in. The memtable format is very plug-in. By default, it's kind of a skip list. But you can have a vector memtable. You can have a list. array memtable for memtable formats. For SSD formats also it's quite pluggable. I can explain in a future slide how there are different types of SSD formats. The reason, again, is because of Facebook, they're like, probably 50 different use cases that uses RocksDB. And to get their engineers who try to get the best performance out of the system. So this is the reason why a lot of things are pluggable. The defaults are a good choice, but If you really want to get a lot of mileage out of your hardware, you might want to experiment with different things. take for example one logging I remember there was some people who are writing like a replication layer on top of RocksDB, right? Because RocksDB doesn't have replication. So what we did was, this is the active memtable, that's the log. And when a write request comes in, you have put K1 and V1, and you also have an opaque parameter that you can write to every transaction login. So it's kind of any opaque thing, any string byte that the database doesn't interpret, What the database does is it writes this K1, V1 to the memtable, but it puts this opaque thing to the library. So now there is a log replicator which reads from the log and tails it and then upload some other cluster in a different like data center for example this is what i'm by customizable. You can actually piggyback a lot of extra data that helps you keep your upper-level customers Caches in your application layer consistent because you can plug in anything there you want There's more description about this. I tried to kind of summarize some of these files. Static sorted tables and all keys are sorted in the static sorted table. They're block based format format, which means that all the SSD files are broken up into individual blocks of, by default, 4K or 8K. But you can have different SSD files, you can have different block sizes. The block-based format is mostly used on spinning disks and SSDs, and there's also a table called plain table format, which is mostly used when data is in RAM. So if you're running RocksDB on RAMFS, RAMFS is like a random access device, right? So you can actually configure the plane table format because it has less CPU overhead when you're using the plain table format. This is what I meant by a lot pieces are pluggable and when you run it on different hardware or different systems you might want to configure these things differently. Any questions so far? OK. So the Bloom filters, a lot of people have asked me questions like, where do you exactly So that's why I created this slide. So the blooms, so there are many databases systems which actually have blooms on SSD files only. Whereas in RocksDB, we have blooms even on keys in the mem table. Because there are use cases, again this is configurable, by default it doesn't have it. But there are use cases where our mem tables are big, and so looking up a sorted skip list was more CPU than using the bloom first. So there is support for boom bits for memtables, too. And obviously, like standard database technologies nowadays, as blooms for SSDs. I wanted to mention this, especially for use cases where you have, where a lot of your data is in RAM or your active mem tables are very big in size for those use cases setting of enabling blooms on mem tables might help reduce CPU cost. Then, about the memtable formats itself, by default, the memtable format is a sorted is a skip list but you could also have an unsorted mem table and what happens is when the write comes in, it just puts it into like a bag of memory, which is not sorted. because you don't support reads from that memtable. So reads are only from data from SSDs, and this is unique. used to bulk load systems bulk load data into rocks tv take for example you have 100 gigabytes of data to upload and nobody's querying Rocksteady at that time so you don't really need to do key by key insert and keep sorting it. Just makes your system easy for bulk loading, for example. you can configure it using an unsorted memtable. Does it make sense? Yeah, question? Yes, same as vector memtable, right. Basically, it's a vector of memory bytes. Okay, column families. So this was another This is the thing that we kind of enabled very early on when we tried to put put Rockstep in production. There were use cases where people were storing two different types of data in the same RocksDB instance. And these type of data needed different compaction strategies different block sizes. Take, for example, people are storing, say, user information and then his friend list or something like that so which is much much bigger than, say, the user information. So I'm just giving some dumb examples, maybe. But basically, two different types of data that you need to store in the same database. And the reason they need to store in the same database is because they want to use atomic rights so that both the keys are there or none of the keys are there. You can't store it on two different databases. So column families, what they do is they behave like exactly two different databases, except except that the transaction log is shared, which means that they give you atomic reads or atomic rights for these databases. So you could configure this column family saying that I want to, I have, you could also put this column family entirely on disk and that column family and SSDs, for example. You could have completely different block sizes and key values on these two different column families. But it shares the same transaction log so that if the database crashes, the consistency between the data that you inserted between those two things are still maintained The write-ahead log, people also ask me questions about what kind of durability does RocksDB provide? Do you lose data just because you've not configured it correctly? So there are features where there are configurations in RocksDB which lets you do three different modes of durability. So if you said disable while equal to true, that means there's no write head log, which means that obviously write is reduced because you're not writing the right head log. But on the other hand, if your machine dies, obviously you lose all the data that is in the memtable. So some use cases it might be okay. And then there is an option where you can say sync equal to false for every write. And What it does is it basically flushes the transaction log to the OS, which means that if your process dies, you still get your data back. But if the machine dies, you might not get the last minutes of writes. And then there's one more option which is sync equal or true, which means that every Every time RockDB writes to the transaction log, it flashes and syncs the transaction log to stable storage. so that you'll get all your data back, even if you crash. Which means the latency might be higher here. Latency will be lower here. And here it will be far less. Your write amplification will be reduced by one because you're not writing the transaction log at all and I've seen all the three more being used at Facebook by three different use cases just because they needed different kinds of reliability. durability. That is as far as writing the transaction log. Now, for reading the transaction log, again, there are, like, three different ways of reading it. One option is that you can say that I want to make sure that when the database restarts, it gathers all the data from the transaction and applies it. And if it cannot apply it because of some reason, because the disk is corrupted, it should throw it and say, sorry, I can't open the database. You have to manually fix it. That's the default mode. Then there is another mode where you say recover all the data except the last record if the last record in your transaction log is corrupted that's fine so ignore it and carry on it's possible because the system have crashed just when you're writing the last record. And then there are other options which say that, recover up to the first corrupted record in your transaction log, and then ignore all the remainder of the transaction. Again, these cases are needed when you have different failure modes on the hardware that you are using. and then recover all valid records, which is basically try your best. Skip over corrupted records and recover as much as you can. is that this is what I meant by saying some people have complained, saying that over-oxidative is very complicated to use. And I kind of agree because there are too many tunables. There are some Good defaults, for example, here, this is the default, recover all except the last one. That's the default on the system if you don't tune it. But you need to know these things in case you need to use use it in a different way. Any questions so far? So the Block cache, I think I already mentioned it, is used only for reads, and the block cache also It also is sharded, which means that the block cache has locks. on it, obviously, but they're the sharded lock system. So by default, I think it's sharded 64 ways. which means that if you're running it on a 16-core machine, most likely you won't have too much log content when you're using the block cache. It's default sharded so you don't have to do anything. Indexing filter blocks can be configured to remain in the cache. I think somebody asked me this question just now. Yeah. So by default, it doesn't remain in the block cache, but you can configure it to be part of the block cache, in which case, when it's needed, it is going to be fetched from the SSD files. And the block cache can also be compressed or uncompressed. Depending on if you have a lot of CPU to use, then you can probably to keep it compressed. Otherwise, the block cache is only delta-encoded and not compressed. Any questions? Oh, yeah. So you can think about it. as all the keys are sorted in an SSD file. But let's say the keys are, say, 1 to 100. there are ten blocks. So when somebody is looking for key number 50, we need to be able to do a binary search on these blocks. So there is a block at the bottom of every SSD file, which is the starting key and the ending key of each block. So that's what I meant by the index block. Um... So the block cache is very much pluggable. You can write your own code to implement a better block cache if you want. defined API. The default one is an LRU cache. Somebody recently contributed a clock cache which is basically the clock algorithm, so it might work better for some use cases. And it's like I said, it's shared by multiple databases within the same process. And that's a key feature that sometimes people don't use, but I think you should probably use it just because you reduce the fragmentation of your memory. when you use that feature, if you have multiple databases on the same machine. use multiple databases on the same machine? Probably some use cases. compaction filter so compaction is a process where rocks DB takes multiple SSD files and makes a single SSD file or a few SSD files, right? The compassion filter is again user specified code. You can actually run your own code when the database is compacting. And that code can actually drop keys. So as part of you can say that I don't want these keys anymore. So as part of compaction, those keys will get removed automatically. So also things like the compaction filter code can be in C++ also. or Lua now, so you can write code both ways. And a very useful functionality, the first thing functionality was a TTL for database records. So if you specify a TTL in your database record, there's a compaction filter. When it's compacting the database, that, oh, this record, it's time for this record to go away, because it has expired. It's 24 hours of time. to live and it automatically drops it. So an application doesn't have to constantly keep scanning the database and deleting actively records that it doesn't want. There are many other higher level functionality that's used that's been implemented by other people, not RocksDB developers using the compaction filter. I don't know if you guys have like a TTL kind of functionality. then you can probably use some of this. Yes? Yeah, I mean, the value of the TTL has no problem. It's just that you need to have space in your storage system to store two years of data. Otherwise, there's no problem. Yes? Oh, good question. But there is a thing which says that in the earlier versions of this code, there are certain files in your bottom most level in your tree which might not get compacted because nobody's overriding in your these keys. So that used to be a problem earlier. Now there is a fix which says that you can configure something which says that I'm going to do a full compaction once every one month. time. So you are guaranteed that your keys will go away after some higher level or some co-op course are granularity. Oh, yes, absolutely. Filter to modify values or useful when sometimes you want to drop values or you going to compact values. Take, for example, you want to do some kind of, let's say your key, and you have 10 values, one for every week. and after some time you want to sort them and make it smaller, the value size. You can actually use it Facebook users Facebook. There is one use case where the value used to be a a long link list, and the link list keeps shrinking in time because they don't need to keep data in so much fine-grained similarity when the data becomes very old. Does it make sense? So that's the compaction filter. And then there is another pluggable thing that developers typically leverage is something called the merge. records. So merge records are, instead of doing a put, you write a merge instead of writing a put record, you write a merge record into the database. And the database knows that this merge record needs to be merged with previous versions of this merge record and it merges it using your code that you specify using a merge operator. So this is, let me explain it, how it's used. then maybe it's easier for me to explain. So it is used mostly we say for counter updates, right? So there was a use case where we were measuring how many times people are clicking on an URL, and that's a counter, and we are using HBase very early on, and so that's every read is like you read the old value of the counter and then you add one to it and then you write it back. So for RocksD be implemented using merge records, which means that you just write a merge record with a value of one every time. you see a click and then you specify a merge operator saying that this is how multiple merge records into one merge record by adding them up. And I remember like a 500 node, HBase cluster, which was doing counter updates, got replaced with a 32 node RocksDB cluster. at that time. This was like 2012 or something, or 2013. So HBase must have improved a lot. So I'm not trying to say bad things about edge base. It's just that at that time, this was a very powerful feature It basically eliminates read-modify rights on your database. And then there is more complicated ones. Some of these merge operators could be associative merge. just because depending on your data structures, like if it is a linked list, it could be It's very easy to do. You should read up on associative margin, generic margin, if you plan to use it. There are also good ways to add external tables to RocksDB now. This is a new feature that has gone in the last one year. So there are times when we have lot of data like pending in Hadoop systems and we want to put it in RocksDB. So what we do is we create a SST fund in Hadoop itself. And there's a Hadoop, like a MapReduce job, actually create an SSD file and then we take that SSD file and add it to rock DB by using a new API called addSSD file. So what it does, it basically takes all the keys in the RSS file and just plops it into the database. It's a very good way to kind of bulk import data from HBase or Hadoop systems. Yes. Yeah, great question. Yeah, yeah, so the question is, how does it interact with keys that are already in the database, right? Because now we have a whole bunch of keys suddenly getting inserted. So there are two modes of adding these keys. Let me So what happens is that every record in the database, This is the interesting part, is that every record in this file gets added as if it was the oldest keys in the database, which means that if there are newer versions of these keys, the thing that you added in the file won't be visible in the keys. Does it make sense? one operation, or you can add it as the most recent version, which means that it will appear You added these 100,000 keys in one shot into the database, and there's the latest version of those keys. You can't really insert someone in the middle of your sequence number space. Does it answer your question? Yeah, the insertion is constant time, it is just a file name swap. So the file actually is, it's not like you take, read every single key from the thing and insert it. We basically take that whole key and swap it into the database. But those keys will appear either as if they have just been inserted into the database or they would appear here as if they were the earliest keys. So if you had overwritten some of these keys, those keys won't be visible anymore because they So the newer versions are the ones that will be visible. Yeah, question? Oh, yeah. Excellent question. So there's an entry in the mem table. Yeah, OK. So how do you handle the case where if there are mem entries in the same for the same key range so we actually look at the memtable entry and then and you flush the memtable to disk so that your version becomes the latest version. And you can also do this on an online database, so you don't have to shut down your database to do this work. Am I running out of time? I couldn't see the clock, so I wasn't sure how it was doing. compression option and storage. What are different ways to compress data? very crucial question because most of the data was on flash and so if we can optimize based on flash, that's a big cost and money savings for us. And so the compression by default is it's per block and so you can configure different block sizes you can say block size is 4k or 8k or 16k or even like 64k bigger size blocks so you get more compression if your block size is bigger, but on the other hand, your random read performance might not be great because for every every random read, now you need to fetch 64 kilobytes from the storage. So you'd have to trade off this based on your own workload. Again, the compression algorithm is very much pluggable. So if there are newer compression algorithms people have in mind, it's very easy to plug it into the system. By default, these are the ones that are supported by the system. Snappy is what came out. with level DB. Zlib and LZ4. Oh, the Z is missing that Zlib and lg4 are also widely used ZSTD is a new one that got added recently. So ZSTD essentially gives you... almost as fast compression as Snappy. But the sizes that it creates is far better than what Snappy does. So you can take a look to see. There's widely published benchmarks. So now a lot of Facebook database, data stores have moved on to GMS. ZSTD. And also there's another which you can configure. You can have a dictionary for the whole SSD file. See, I said compression is per block. But typically, different blocks might have still shared strengths and objects. So it's good to have a dictionary for the whole SSD file. So there is an option now that you can specify the dictionary size and the dictionary gets created when you create the SSD file as part of compaction. That's a useful feature. I have seen that quite useful, especially when somebody was asking me about inverted indices, because some of those text things really are the same text fields in different blocks. And so if you use a dictionary per SSD file, I have seen a lot of significant space saving. dictionary size is configurable. Again, RocksDB is optimized for short range camps. What does it mean. So there are lots of use cases we have seen where a lot of applications just seek to one key and then they want to fetch the next 50 keys. Very typical use case in Facebook, like Find Me, My top 100 friends, for example, right? Or find me the top... or the latest pictures that I've seen, only the last 10 pictures that I've seen. So all of those are very shocking short-range scans. And so we have optimized DroxDB for kind of short-range scans. especially range scans which have the same key prefix. So we've organized this data so that the key prefix remains the same, and we can do quick range scans on key prefix. We have also now blooms for prefix. So typically LSM engines don't have great support for um don't really use blooms when you are doing scams because there's no no way to use blooms for rocks db what you have done is you can create blooms per prefix instead of per key. So now we can actually leverage Blooms for short range scans. And that was also a very powerful feature to reduce read amplification. Does it make sense? So, a little bit, if I step back, the development of the Development started in 2012 of this, and 2013 is when we open sourced it. There were a only probably three use cases at Facebook, three or four use cases at Facebook in 2013 when we opened sourced it. But it's only after the open sourcing is when we saw a real lot of adoption by different people. One of the first teams that I worked with to see if it is usable out side of Facebook was LinkedIn. They kind of rewrote a lot of their feed infrastructure based on RocksDB at the time. Then Microsoft Bing uses RocksDB just for their Bing platform and then a plethora of other storage systems. there is an iOS and Android port. I don't know how many people are using it. It's called RoxDB Lite. So when you build it with the RoxDB Lite flag, you actually get a much smaller shared library that you can use on Android. and iOS too. But it is not used inside of Facebook, so I don't really know how great it is or how popular it is and then of course my SQL and MongoDB story Do you want to hear a little bit about the MySQL story, how it moved to... to to to rocks to be instead of you know to be I have probably a slide so yeah so So the first experiment, again, I think 2014, is when we looked at MongoDB. was used by a system called Parse that Facebook acquired. Parse was a big user of MongoDB and there's like terabytes of storage when you put it in MongoDB with a RocksDB storage engine It was reduced to 285 gigabytes. It was a huge difference at that time. has improved a lot since then. There's not going to be five terabytes anymore because this is what puts pressure on like commercial vendors saying that, hey, look, we need to make things better. a RocksDB storage engine inside MongoDB, so when you run MongoDB, you can actually say, hey, I want to store RocksDB inside it or store it in RocksDB database. Similarly, RocksDB storage engine for MySQL. So Facebook uses MySQL a lot. This is where the Facebook graph is stored. And so it's like basically really real-time queries, key values, and again, short-range counts. There is a benchmark called Link Bench, which basically replicates the Facebook Graph workload. source benchmark. And then when you run it on benchmarks, we see that the size of the database is far small and rocks to be. So this is the benchmark that really convinced a lot of people at Facebook saying that, hey, we should use We should evaluate RocksDB to see if we can use it for our graph database. This is not the decision point. this was mostly a trigger for people to think that, okay, if we reduce the cost by 50%, what are we giving up? Are we giving up on read latency? Or are we giving up on write amplification or something else? But people started to investigate this. And then very quickly saw that even write latencies are actually better because we write far fewer bytes to disk. So there are side effects. And again, this didn't happen for free. There was lots of work by the RocksDB team to make this happen. There were like eight or nine people working on this to make sure that we can be far better than in order to be at the time. We can forget about this one for a second. theories. So the two features that essentially my SQL needed from RocksDB are transaction support. So these are two features. that RocksDB has built. One is optimistic transactions and one is pessimistic. So optimistic transaction, the way it works works is it basically gets a list of keys that your transaction is touching. And when you say commit, it verifies that you have not actually modified any of these keys. And if you modify it, then the transaction will fail. Not a great system, but it was mostly a proof of concept at the time. Now we also have pessimistic transactions, which is what is used by MySQL. So again, for MySQL, what happens is that we at Facebook use MySQL because MySQL has replication support, MySQL has great backup support, And we have like 20 or 30 database engineers who know MySQL very well as far as operations has come. So we can't really move our whole graph database to a brand new system called RocksDB, which which you have to build all these things from scratch. So we kind of tried to get the best of both worlds. We put RocksDB in inside the MySQL wrapper, so that operations and everything remains the same, except that well, instead of using InnoDB, they use RocksDB Storage Engine. So that's, again, an open source product, and I think Pinterest or somebody else also uses it, I think, now for storing some of their graph database. but it is supported by Parkona and MariaDB and other database vendors now I can tell you more about MySQL after this if you have more questions. focus much on the MySQL side of the story, but that's kind of a very powerful story because that's kind of the crown jewel for the rocks for Facebook data store, which needs durability as well as availability and latency and that's after production. What should we expect? Yeah, I think Facebook, the ground database is probably like in double digit petabytes. So it is very difficult to kind of migrate it. And there's a lot of process that I can talk to you after this Yeah, sure, sure. Any other questions? Iteration means like scan? Oh, yeah, excellent question, yeah. Yeah, so the question again is that how does Bloom filters work with prefix scans? So what happens is that if you, in a traditional key value store, You can have an operation calling seek to a key and then call next next next. next so it keeps on going to the next because all of it is sorted right so in prefix scanner what you do is that you call seek but you specify prefix which means that it goes to the first key which has that prefix and then you can keep calling next but it will give you keys only with a same prefix. So once you run out of keys with the same prefix, it doesn't give you anything. So that's the difference between prefix scans and normal scans. Normal scans we keep going till the end of the database. Whereas prefix scans, you can scan keys only with that same prefix. And so So now the bloom filters, instead of having a bloom per key, we have a bloom per prefix. So when somebody says, I want to scan this prefix, and I use the Bloomfield code, I can quickly check that out of my 20 SSD files, 19 SSD files doesn't have this prefix. So I don't have to read them. That's the optimization. Thank you. Oh, no. So, no. So, the prefix, so the way you do it, you, write some code which says that this is how I extract my prefix from my keys. You can also say that there's also some fixed ones. You can say my prefix is the first 10 bytes of every key. Those are the simple algorithms, but it's pluggable You can say that the database is going to call into your code saying that given this key, is its prefix and you return the prefix as part of the pluggable code. So people at Facebook, they'll put some user ID, friend ID, something else concatenated into one key. And so some of those are fixed ones. Some of them are variable size. . Yes, correct, correct. So that's the standard iteration process. Yeah, there's a heap and then that iteration The iteration code is actually pretty complicated now because it needs to iterate with different assistive files, key overlaps and sequence number overlaps. And also, RocksDB has something called snapshots that I didn't talk about. much. So you can have a snapshot, which is basically any point in time snapshot of the So if you have a snapshot, you cannot compact across snapshots because you'll lose the consistency of a snapshot. That code is quite complicated now. way more efficient than some kind of range index Is Bloom filters more efficient than range index? Yeah. Like, . Yeah. Range indexes are here. I mean, range indexes are here. I mean, range indexes are here. indexes don't tell you whether the key is in the file or not. It tells you that a key, if you're looking for a key, and it fits the index, you have to go open the file and check inside it. So the index needs less... memory, the bloom filters need more memory, but you have absolute guarantee that your key is not in this file. So, you have a question? Yeah. Mm-hmm. Are there any fancy features that kind of mitigate that? Yeah. So there is a... Yeah, so the question is that, how do you handle deletes? Because tombstones might actually cause inefficiency in your LSM tree. So there are two ways. action process right now, it actually looks at the number of deletes in a file and figures Basically, it's one more metric to figure out which to compact first. That's one point. The second point is there is a new feature that got developed in the last year which is about range deletes. So you can actually delete a bunch of keys using one delete key. So you don't You don't have to delete each and every individual key. You can look range delete for aux.db. Go to wiki page now. That's the second one. And the third one that comes to my mind is something called single delete. So single delete is another option. optimization where you can use single delete if you know that a database already has only one version of the key. Suppose your application is such that you already know that there is always one version of your key, then you can use single delete and it'll remove the tombstone right then and there as soon as it merges with the first record. So those are the three things maybe you can take a look and see if that helps. Question? . and if so, what are the factors that's causing them to like, you know, No, I think Facebook, everything has moved to... So as far as I know, probably around 80-90% have already moved to RocksDB, and there might be, so Facebook has something called the Facebook timeline. which is basically you can go to your timeline and look at all your old things. Some of it still runs on disk, so I'm not sure whether that has moved to ROXDB or not. But the focus of the whole company is to move everything I think the rock's to be. No. There's a detailed post now. You should look at the post from Yoshi. He's the guy who is handling the Myra And he has a very detailed post on how RocksDB with MySQL is better and better. on all the fronts, not just one. So this is what I mentioned. In the beginning, it was mostly a size efficiency. And all of us thought that we are giving up on something else. But for all the faith leaders, Facebook use cases now, because of the work of all the team, of the RocksDB team is that it has improved on all three sides. Latency, right amplification, as well as size of the database. I am not 100% sure about the use case on disk. Like I said, the timeline is still on disk. But I think if you poke around on the Google search, you'll probably find find something there. We have almost run out of time. So I can help answer any other questions later if you have more things. I think we are at time, but welcome to stay back so you can huddle around and ask questions. Let's give a round of applause to our speaker. Thank you. Welcome to GetFood. and then we'll hang out and you'll stay for more questions. Thank you. Is there any questions I'm I'm happy to answer them. Yeah? What's the biggest size Oh, that's another good question. So I think all the benchmarks that we have done, it seems to me that the values are around 1k to 4 k, the value size, is when you get wrong is better than Inonib. If you're looking at value sizes, where the values are very big, 80k or more then it's possible that there is right amplification that drops to be is not the greatest solution for. In those cases, so Facebook has reduced cash flow. called memcache. It stores a lot of data. And memcache has a rocksdb cache now. because they want to extend the cache into SSDs. So memcache, the key range is quite high, quite broad, but I think 90 percentile use cases more than 80k yeah so for those use cases now rocks DB has compaction mode which is called blob store so for this the blob store mode reduces write amplification. I didn't talk much about the blob store part yet, But again, this is basically because 10% of the keys are more than 80k. Instead of using level compaction, you use blob compaction. And I think the sweet spot is the smaller the keys and values, the more is the performance. performance difference between RocksDB and other systems. When the keys become, or the values become bigger bigger, then the performance difference might be lesser and lesser. Any Any other questions? You talked about using ROPS TV as a MySQL back end. also as kind of using it as an individual in like individual services um storing specific service data. How do you decide between those two different modes of deployment? Well, good question. I think when Facebook started, the only thing Facebook had was the Facebook graph. Right? So this is good. 2006 or five, they basically set up a sharded instance of MySQL. and say that user data, user posts, user likes will store here. At that time, it was all about just storing and serving the user data. It's only, I think, 2008. or something, or 2009, when we started to think about monetizing this stuff, and this is when all the entities and then the conciliary databases came up, saying that how can we figure out better ways engaging people, people you may know, how can you improve a lot of your... correlations between users, how can you show relevant posts for users like a relevance engine. So this is where you take the Facebook graph and then you crunch and put it in a different form because that's the best way to serve those queries. And all these things are essentially ancillary databases are very tightly coupled to the application, and they're embedded databases. Am I making sense? So the Source of Truth is kind of the graph database, but that's not the one that you use to recommend friends that you might like, for example, right? So you need to do a lot more processing. So all these things. is how I call as secondary indexing, but not at the database layer. It is more like secondary indexing for your applications. So, like, say, take, for example, you want, you go to a location, and you want to find out all the photos taken by your friends at that location. Yeah? So you don't Don't pre-calculate all these things. You have to get it right then and there when I say check in. So this is what I meant by, and you don't need to get your latest data from your graph database. Okay if the data that you are serving for that type of query is, say, one hour old, which means that now you need to build all these second indices and different ways of looking at the same data. And this is where Rockstab is used a lot. It's kind of similar to some of your search use cases, I think, where You need kind of a different way to look at your same data set. Question from there? Yeah, I think it was like a two-year effort from the From the date it was decided that we should migrate all the stuff to the time now where probably 80 or 90% migrated. The biggest issues. A lot of it was building systems and verifying them, like the backups are correct, and then how do you actually do replication. Is replication working well when you're running with rocksdb instead of inodb? then there is validation of data. So we have, at Facebook we have a good of shadowing our use cases. So what we do is that we take, so let's say we'll take these 20 miles boxes and say we'll shadow these MySQL boxes. So the same write traffic is going to two places. And then when a read request comes, that read request is also going to go to both these places. And as part of the read code, which is basically HSVM or PHP or... just think about it just as an application process. It has an asynchronous way of fetching data from the shadow and validating that the data is still the same. So it is not like a single team project. It's a multi-team project because lots of instrumentation has happened even in the PHP code or the application code which can validate the shadowing of databases. And this did not happen. So the first time this was actually useful I was in a project called UBase, and I was trying to replace MySQL with HBase. I spent around nine months on that project. And this is when other people in the team helped me build the shadowing framework. And then I stopped that project saying this project can't go much much because HBase has a lot to catch up. And that's when I started to develop RocksDB. But yeah, the challenge in my mind is not the code, it's more about the deployment process and keeping it running without any corruption and without any big fire so now I think it will be lesser. Again, I think I would caveat it with one thing thing. In my mind, I think what I see is that if you try to replace a system, the first thing I would be doing is to make sure the people who are running the existing systems are the guys who are doing the migration. This is more, how should I say, social engineering rather other than some technical thing. It's a different team cannot be doing the migration. Because these are the guys who actually know how your system runs. And once they're convinced that your system is better, that What is the time when you think that, okay, this project is going to be successful? If you put in five different people saying that, okay, we have this. system now let the old people continue to run that system when the new five guys try to replace that system. That's very difficult in my mind. Just because of social engineering issues, it's kind of become you and me are not easy to do So again, I think the thing when I thought My Rocks was going to be popular at Facebook was when like the 25 people, MySQL operations guys, said that yes, we are going to make this work. I knew that because they know MySQL inside out. They've been running it for like eight years. And then The challenge was that for ROXDB itself, we basically became... slave and they were the masters. They were saying to us, you need to do this in Rocksteep. You need to reduce right amplification so it goes do that stuff. And they do the integration and actually shadow it and test it. It's not us, like outsiders in my mind. I'm an outsider, I don't know how to run MySQL. Oh no, so the MySQL operations team is around 20. So maybe two guys are the guys who really worked on No migration. But I know Facebook has published a lot of . So Facebook actually does I developed a lot of MySQL code. For example, MySQL in ODB compression was done in Facebook. earlier in ODP did not have compression. So there's a Facebook, one Facebook engineer did this. But none of the Facebook engineers who worked at the InnoDB code they felt that it was very difficult to... do compression well. Just because in a B-tree you'd have to like say it's an 8K page or 16k page. It's not like flexible page size. So you have a lot of defragmentation and you lose a lot of space. So there was one engineer who used to know InnoDB code inside out and the others were mostly like small time changes to inaudible Any other question? When collecting metrics from, like, energy, it actually costs... . . So, RocksDB is actually there's an API to fetch metrics. The metrics are all they're kind of lock free metrics all of them But still there's an overhead. If you run that thing like many times then I don't know how well it will be, but all those things have been changed to lock-free data structures. So it depends on how frequently you access. We also have inbuilt histograms and stuff like that. So you can look at statistics.h, there's a header file. That's the public-facing API for OXDB. The way people consume this metrics are not directly using that API, but mostly, like, Facebook has a scraper, right, which runs like metrics collections on every node. And RocksDB runs as part of the search backend. Search has a gRPC or Thrift or some other API that the collection The metrics collection agent will poll, and then the server will fetch this from RocksDB and give it to the metrics database. Do you use any open source metrics things like Prometheus and... and all this new open source? No. All in-house. OK. question . . Yeah, good question. The first thing that I did was that the slave, a few slaves, started to run my rocks. No, actually before that. So there are MySQL instances that are used only for testing. Those were the first ones that got translated to MyRocks. Those are basically like the nightly tests and hourly tests and all that that they use, they go over there. They also run on normal MySQL boxes, but they also run on this. And then the second thing, again, The second big thing is that a lot of slaves, or like 80% of slaves, got migrated to Malayalam. And as part of that, there were two slaves and one master for those. And one of the slaves was Marduk. Myrox. And two of the slaves are Myrox. And then when the switchover happens, that's when the Myrox becomes a master. This is what I meant by that this is all operations driven. This They already had scripts to make slaves a master. When to make sure? They have to make sure the master is dead. sure that all the bin logs are replicated. So we still use, MyRoc still uses bin logs. So it uses basically my mysql replication mechanisms I don't really know. I mean, I am sure we have run into it in the sense that this is all new software, but I don't remember one lightning strike or something. Those all being part of the testing. . Yeah, I think I am not 100% accurate maybe, but probably around the My Rocks database is around 20 petabytes. Um, and the requests will be like billions in seconds, billions per second, I don't know how much. That's like, if you're look at the tau, there's a paper called tau, the tau paper, which is basically the graph database for Facebook. There I think there are some numbers, but those numbers are like four years old, so I really don't know what is a read request. Oh, outsider Facebook, we have seen commercial support from Parkona now, so they actually support MySQL with MyRocks. And I know one or two financial companies use it. But I don't really know who else is using it. Percona has been pushing this a lot because they see a lot of uh And also because Facebook development team is quite big in RSS, RocksDB compared to the InnoDB development team now. So I think over time, Things will just become more and more better. Yeah, question? We've recently discussed the think and strategy. Yes, so absolutely. Absolutely, yeah. I think... Without batching, things don't work. So what happens is that if you do transactions from multiple threads, I should not say transactions. If you're writing to a transaction log, they get batched up. the batch writes the whole pending amount of transactions to their wall and the other guy block there and then when the transaction finishes we free up all the transactions that have been committed. And then the next batch goes. So this is what I meant by saying the reads are highly scalable. So if you have 8 CPUs, you get something reads per second. If you use 16 CPUs, you probably get around close to twice the reads, which means reads are more or less linearly scalable. The writes are not linearly scalable, which means that if you have eight CPUs, you are getting some amount of write power. per second if you add eight more CPUs you won't get double the amount of writes it's not of linearly scalable. So there is more work to be done. The recent work that has gone in is that the rights to transactional and rights to the mem table are in parallel now. But I think there's a lot more work to be done rights go to some ungodly number. Yeah. Yeah. How do you actually perform on the lowest level? How do you perform in read or write? because all these assess tables are basically mutable. You can freely map them into memory or you can Yeah, so rocks could be reads portions of these files and puts it into... So every file is broken up into blocks. And so when you're looking for a key, RocksDB looks at the index and figures out which block to read. and then reads the block from the storage to the block cache. No, it's not a map. So we have also, there's also an option to do mapping and you can benchmark it but we don't use mapping for most of our use cases just because the performance is bad. So yeah, you can do mappings also. What was I about to say? Yeah, so also, RocksDB uses fadvice calls to be able to manipulate the OS cache and make sure the OS cache is wisely used. Things that in the block cache that get removed from the OS cache, vice versa, depending on your config again. The difference though is that the worst cache, the data is compressed, and the RocksteadyBlock cache, the data is uncompressed. So you'd have to think about that when you try to see which cache is best. There are some use cases that Facebook uses only the OS cache and a very small block cache because they have a lot of data and they want to keep it cached and it fits in an OS cache. But there are some other use cases which really really low latency so they have very big block cache. So, yeah, the question is do we use ODirect? It's a new feature that has come in in the last one year. I believe there is only one use case that uses ODirect right now. So this code definitely production ready, but I think we just need to do more. I mean, whoever uses it needs to do a lot more testing than the other pieces of code. Yeah, you had a question? Yeah. . How does it work? Is that a test? How does it work? Okay, yeah, good question. So the block size is configurable. You can say block size is 4K, 8K, 16K. whatever it is, but it's a heuristic, which means that if there are keys, so understanding Unlike InnoDB, if your key is, let's say you're filling up a block with keys, and values and the last value is maybe two bytes more than the block right so we are going to put those two bytes in a different block. So it's actually every block in RocksDB's variable size. specify just a heuristic so you kind of keep it close to that size. So if you have a a value which is humongous, it will be one block with that value. me . yeah so So what will happen is that, let's say their block size is 4K. So now what will happen is that, This is good. 4K is only heuristic. So let's say you have 5,000 byte value. That 5000 byte will be one block, and it will be one read from there will be only one decompression entry for this it will not be two blocks so decompression be only for it will be done only once So the value never crosses multiple blocks. The block gets extended. to the next value question So, rocks is like infinitely tunable and has an infinite array of plug-ins. . . Is there like a documented kind of default configuration or a well-tested arrangement Yeah, I think there are two options. So Facebook has an open source branch, which has Most of the MyRox changes, but that is not the branch that most or the financial companies are using. They're using a branch from Parkona, which is basically a SQL redistributor and Percona actually takes the Facebook MySQL branch and actually puts it in my SQL. So I think in my mind, I think there are two options. for a company like Dropbox. One obviously is to go talk to Percona. The other option would be to say that I will take the Facebook MySQL branch and then try to work with the Facebook developers and see if that is a faster process. In my mind, I think the, actually I don't know why, what will work better? This question hasn't aroused before because I think the Pinterest guys, Please don't quote me. Please check yourself. I think they mostly go with Percona. Because that's how their other MySQL boxes are also being used. They have support from all Right. . Right. Yeah, I agree. I can put you into... in touch with the MySQL operations folks at Facebook, they I think will be able to give a better answer to this question other than me yeah I can do that please check with Brian and he can put me in contact or Min and he can put me in touch with you any other questions Yeah, two things. One is right scalability and the other one is tiered storage. Those are the two basic focus. Tiered storage is where you have spinning disks and SSDs and NVMEs and RAM. So four kinds of storage systems on one node. So how can the database leverage this and make sure the right things are in the right layers? So that's one focus. Again, because the databases are becoming bigger in size, and putting everything in RAM or NVMe or even SSDs now is a cost factor. So that's one. And the second one is how can you do more writes? The Facebook graph database is actually 99% reads and 1% writes. But there are other use cases that Facebook that uses Rocksteady where there are a lot of rights. So those are the two main factors. focus of the team. Cool. Okay, great. Thank you Thank you. Thanks a lot. Keep in touch and let us know how we can help in any other way. you